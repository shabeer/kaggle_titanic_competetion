{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial version of machine learning algo for titanic kaggle competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PassengerId  Survived  Pclass  \\\n",
      "0            1         0       3   \n",
      "1            2         1       1   \n",
      "2            3         1       3   \n",
      "3            4         1       1   \n",
      "4            5         0       3   \n",
      "\n",
      "                                                Name     Sex  Age  SibSp  \\\n",
      "0                            Braund, Mr. Owen Harris    male   22      1   \n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female   38      1   \n",
      "2                             Heikkinen, Miss. Laina  female   26      0   \n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female   35      1   \n",
      "4                           Allen, Mr. William Henry    male   35      0   \n",
      "\n",
      "   Parch            Ticket     Fare Cabin Embarked  \n",
      "0      0         A/5 21171   7.2500   NaN        S  \n",
      "1      0          PC 17599  71.2833   C85        C  \n",
      "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
      "3      0            113803  53.1000  C123        S  \n",
      "4      0            373450   8.0500   NaN        S  \n",
      "       PassengerId    Survived      Pclass         Age       SibSp  \\\n",
      "count   891.000000  891.000000  891.000000  714.000000  891.000000   \n",
      "mean    446.000000    0.383838    2.308642   29.699118    0.523008   \n",
      "std     257.353842    0.486592    0.836071   14.526497    1.102743   \n",
      "min       1.000000    0.000000    1.000000    0.420000    0.000000   \n",
      "25%     223.500000    0.000000    2.000000   20.125000    0.000000   \n",
      "50%     446.000000    0.000000    3.000000   28.000000    0.000000   \n",
      "75%     668.500000    1.000000    3.000000   38.000000    1.000000   \n",
      "max     891.000000    1.000000    3.000000   80.000000    8.000000   \n",
      "\n",
      "            Parch        Fare  \n",
      "count  891.000000  891.000000  \n",
      "mean     0.381594   32.204208  \n",
      "std      0.806057   49.693429  \n",
      "min      0.000000    0.000000  \n",
      "25%      0.000000    7.910400  \n",
      "50%      0.000000   14.454200  \n",
      "75%      0.000000   31.000000  \n",
      "max      6.000000  512.329200  \n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "\n",
    "# We can use the pandas library in python to read in the csv file.\n",
    "# This creates a pandas dataframe and assigns it to the titanic variable.\n",
    "titanic = pandas.read_csv(\"titanic_train.csv\")\n",
    "\n",
    "# Print the first 5 rows of the dataframe.\n",
    "print(titanic.head(5))\n",
    "\n",
    "print(titanic.describe()) # Describes only numeric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    714.000000\n",
      "mean      29.699118\n",
      "std       14.526497\n",
      "min        0.420000\n",
      "25%       20.125000\n",
      "50%       28.000000\n",
      "75%       38.000000\n",
      "max       80.000000\n",
      "Name: Age, dtype: float64\n",
      "count    891.000000\n",
      "mean      29.361582\n",
      "std       13.019697\n",
      "min        0.420000\n",
      "25%       22.000000\n",
      "50%       28.000000\n",
      "75%       35.000000\n",
      "max       80.000000\n",
      "Name: Age, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Before filling missing values\n",
    "print(titanic.describe()[\"Age\"])\n",
    "\n",
    "# Filling missing values with median of values.\n",
    "age_median = titanic[\"Age\"].median()\n",
    "titanic[\"Age\"] = titanic[\"Age\"].fillna(age_median)\n",
    "\n",
    "print(titanic.describe()[\"Age\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['male' 'female']\n",
      "['male' 'female']\n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "# Find all the unique genders -- the column appears to contain only male and female.\n",
    "print(titanic[\"Sex\"].unique())\n",
    "print(titanic.Sex.unique()) # Unique values in Sex column.\n",
    "\n",
    "is_male_array = titanic.Sex == \"male\"\n",
    "is_female_array = titanic.Sex == \"female\"\n",
    "\n",
    "# Replace all the occurences of male with the number 0.\n",
    "titanic.loc[is_male_array, \"Sex\"] = 0\n",
    "\n",
    "# Replace all the occurences of female with the number 1.\n",
    "titanic.loc[is_female_array, \"Sex\"] = 1\n",
    "\n",
    "print(titanic.Sex.unique()) # Unique values in Sex column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['S' 'C' 'Q' nan]\n",
      "[0 1 2]\n"
     ]
    }
   ],
   "source": [
    "# Find all the unique values for \"Embarked\".\n",
    "print(titanic[\"Embarked\"].unique())\n",
    "\n",
    "# The most common embarkation port is S, so let's assume everyone got on there.\n",
    "embarked_default = \"S\" \n",
    "titanic.Embarked = titanic.Embarked.fillna(embarked_default)\n",
    "\n",
    "is_S_embarked = titanic.Embarked == \"S\"\n",
    "is_C_embarked = titanic.Embarked == \"C\"\n",
    "is_Q_embarked = titanic.Embarked == \"Q\"\n",
    "\n",
    "# Replace all the occurences of S with the number 0.\n",
    "titanic.loc[is_S_embarked, \"Embarked\"] = 0\n",
    "titanic.loc[is_C_embarked, \"Embarked\"] = 1\n",
    "titanic.loc[is_Q_embarked, \"Embarked\"] = 2\n",
    "\n",
    "print(titanic[\"Embarked\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Pclass Sex  Age  SibSp  Parch      Fare Embarked\n",
      "297       1   1    2      1      2  151.5500        0\n",
      "298       1   0   28      0      0   30.5000        0\n",
      "299       1   1   50      0      1  247.5208        1\n",
      "300       3   1   28      0      0    7.7500        2\n",
      "301       3   0   28      2      0   23.2500        2\n",
      "297    0\n",
      "298    1\n",
      "299    1\n",
      "300    1\n",
      "301    1\n",
      "Name: Survived, dtype: int64\n",
      "   Pclass Sex  Age  SibSp  Parch     Fare Embarked\n",
      "0       3   0   22      1      0   7.2500        0\n",
      "1       1   1   38      1      0  71.2833        1\n",
      "2       3   1   26      0      0   7.9250        0\n",
      "3       1   1   35      1      0  53.1000        0\n",
      "4       3   0   35      0      0   8.0500        0\n",
      "0    0\n",
      "1    1\n",
      "2    1\n",
      "3    1\n",
      "4    0\n",
      "Name: Survived, dtype: int64\n",
      "   Pclass Sex  Age  SibSp  Parch     Fare Embarked\n",
      "0       3   0   22      1      0   7.2500        0\n",
      "1       1   1   38      1      0  71.2833        1\n",
      "2       3   1   26      0      0   7.9250        0\n",
      "3       1   1   35      1      0  53.1000        0\n",
      "4       3   0   35      0      0   8.0500        0\n",
      "0    0\n",
      "1    1\n",
      "2    1\n",
      "3    1\n",
      "4    0\n",
      "Name: Survived, dtype: int64\n",
      "     Pclass Sex  Age  SibSp  Parch      Fare Embarked\n",
      "297       1   1    2      1      2  151.5500        0\n",
      "298       1   0   28      0      0   30.5000        0\n",
      "299       1   1   50      0      1  247.5208        1\n",
      "300       3   1   28      0      0    7.7500        2\n",
      "301       3   0   28      2      0   23.2500        2\n",
      "297    0\n",
      "298    1\n",
      "299    1\n",
      "300    1\n",
      "301    1\n",
      "Name: Survived, dtype: int64\n",
      "   Pclass Sex  Age  SibSp  Parch     Fare Embarked\n",
      "0       3   0   22      1      0   7.2500        0\n",
      "1       1   1   38      1      0  71.2833        1\n",
      "2       3   1   26      0      0   7.9250        0\n",
      "3       1   1   35      1      0  53.1000        0\n",
      "4       3   0   35      0      0   8.0500        0\n",
      "0    0\n",
      "1    1\n",
      "2    1\n",
      "3    1\n",
      "4    0\n",
      "Name: Survived, dtype: int64\n",
      "     Pclass Sex  Age  SibSp  Parch    Fare Embarked\n",
      "594       2   0   37      1      0  26.000        0\n",
      "595       3   0   36      1      1  24.150        0\n",
      "596       2   1   28      0      0  33.000        0\n",
      "597       3   0   49      0      0   0.000        0\n",
      "598       3   0   28      0      0   7.225        1\n",
      "594    0\n",
      "595    0\n",
      "596    1\n",
      "597    0\n",
      "598    0\n",
      "Name: Survived, dtype: int64\n",
      "698\n",
      "0.783389450056\n"
     ]
    }
   ],
   "source": [
    "# Train and make predictions\n",
    "from sklearn import cross_validation\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "model = LinearRegression()\n",
    "seed = 1\n",
    "num_instances = len(titanic)\n",
    "#print(titanic.shape)\n",
    "#print(num_instances)\n",
    "#print(titanic.shape[0])\n",
    "num_folds = 3\n",
    "\n",
    "kfold = cross_validation.KFold(n = num_instances,\n",
    "                               n_folds = num_folds,\n",
    "                               random_state = seed)\n",
    "\n",
    "# The columns we'll use to predict the target\n",
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n",
    "\n",
    "\n",
    "predictions = []\n",
    "# train and test contains indices of train and test data in each fold\n",
    "for train, test in kfold:\n",
    "    train_predictors = titanic[predictors].iloc[train]\n",
    "    print(train_predictors.head())\n",
    "    \n",
    "    train_target = titanic.Survived.iloc[train]\n",
    "    print(train_target.head())\n",
    "    \n",
    "    test_predictors = titanic[predictors].iloc[test]\n",
    "    print(test_predictors.head())    \n",
    "    \n",
    "    test_target = titanic.Survived.iloc[test]\n",
    "    print(test_target.head())\n",
    "\n",
    "    model.fit(train_predictors, train_target)\n",
    "    test_predictions = model.predict(test_predictors)\n",
    "    \n",
    "    predictions.append(test_predictions)\n",
    "\n",
    "# The predictions are in three separate numpy arrays.  Concatenate them into one.  \n",
    "# We concatenate them on axis 0, as they only have one axis.\n",
    "predictions = np.concatenate(predictions, axis=0)\n",
    "\n",
    "#print(predictions)\n",
    "\n",
    "# Map predictions to outcomes (only possible outcomes are 1 and 0)\n",
    "predictions[predictions > .5] = 1\n",
    "predictions[predictions <=.5] = 0\n",
    "\n",
    "# NOTE: Test set in for first fold starts from index 0 and similarly test sets in other folds.\n",
    "# So comparing predictions and titanic.Survived is correct.\n",
    "\n",
    "#Return (x1 == x2) element-wise.\n",
    "comparison_result = np.equal(predictions, titanic.Survived)\n",
    "#print(comparison_result)\n",
    "equal_results_filtered = comparison_result[ comparison_result == True]\n",
    "#print(equal_results_filtered)\n",
    "correct_predictions = len (equal_results_filtered)\n",
    "print(correct_predictions)\n",
    "\n",
    "accuracy = (correct_predictions * 1.0) / len(titanic)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training accuracy: 0.789001122334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "results = cross_validation.cross_val_score(model, \n",
    "                                           titanic[predictors], \n",
    "                                           titanic.Survived, \n",
    "                                           scoring = 'accuracy', \n",
    "                                           cv=kfold)\n",
    "print(\"Average training accuracy: {}\".format(results.mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 0 1 0 1 0 0 0 1 0 1 1 0 0 1 1 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 0 1\n",
      " 1 0 0 0 0 0 1 1 0 0 0 1 1 0 0 1 1 0 0 0 0 0 1 0 0 0 1 1 1 1 0 1 1 1 0 1 1\n",
      " 1 1 0 1 0 1 0 0 0 0 0 0 1 1 1 0 1 0 1 0 1 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0 0\n",
      " 1 1 1 1 0 0 1 0 1 1 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0\n",
      " 0 0 1 0 0 1 0 0 1 1 0 1 1 0 1 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0 1 1 0 0 1 0 1\n",
      " 0 1 0 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 1 0 1 0\n",
      " 1 0 1 1 0 1 0 0 0 1 0 0 0 0 0 0 1 1 1 1 0 0 0 0 1 0 1 1 1 0 1 0 0 0 0 0 1\n",
      " 0 0 0 1 1 0 0 0 0 1 0 0 0 1 1 0 1 0 0 0 0 1 0 1 1 1 0 0 0 0 0 0 1 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 0 1 0 1 1 0 1 0 0 1 1 0\n",
      " 0 1 0 0 1 1 1 0 0 0 0 0 1 1 0 1 0 0 0 0 1 1 0 0 0 1 0 1 0 0 1 0 1 1 0 0 0\n",
      " 0 1 1 1 1 1 0 1 0 0 0]\n",
      "[[3 0 34.5 ..., 7.8292 2 0]\n",
      " [3 1 47.0 ..., 7.0 0 0]\n",
      " [2 0 62.0 ..., 9.6875 2 0]\n",
      " ..., \n",
      " [3 0 38.5 ..., 7.25 0 0]\n",
      " [3 0 27.0 ..., 8.05 0 0]\n",
      " [3 0 27.0 ..., 22.3583 1 0]]\n"
     ]
    }
   ],
   "source": [
    "titanic_test = pandas.read_csv(\"titanic_test.csv\")\n",
    "\n",
    "# Filling missing values with median of values.\n",
    "age_median = titanic_test[\"Age\"].median()\n",
    "titanic_test[\"Age\"] = titanic_test[\"Age\"].fillna(age_median)\n",
    "\n",
    "# Filling missing values with median of values.\n",
    "fare_median = titanic_test[\"Fare\"].median()\n",
    "titanic_test[\"Fare\"] = titanic_test[\"Fare\"].fillna(fare_median)\n",
    "\n",
    "is_male_array = titanic_test.Sex == \"male\"\n",
    "is_female_array = titanic_test.Sex == \"female\"\n",
    "\n",
    "# Replace all the occurences of male with the number 0.\n",
    "titanic_test.loc[is_male_array, \"Sex\"] = 0\n",
    "\n",
    "# Replace all the occurences of female with the number 1.\n",
    "titanic_test.loc[is_female_array, \"Sex\"] = 1\n",
    "\n",
    "#print(titanic_test.Sex.unique()) # Unique values in Sex column.\n",
    "\n",
    "# The most common embarkation port is S, so let's assume everyone got on there.\n",
    "embarked_default = \"S\" \n",
    "titanic_test.Embarked = titanic_test.Embarked.fillna(embarked_default)\n",
    "\n",
    "is_S_embarked = titanic_test.Embarked == \"S\"\n",
    "is_C_embarked = titanic_test.Embarked == \"C\"\n",
    "is_Q_embarked = titanic_test.Embarked == \"Q\"\n",
    "\n",
    "# Replace all the occurences of S with the number 0.\n",
    "titanic_test.loc[is_S_embarked, \"Embarked\"] = 0\n",
    "titanic_test.loc[is_C_embarked, \"Embarked\"] = 1\n",
    "titanic_test.loc[is_Q_embarked, \"Embarked\"] = 2\n",
    "\n",
    "#print(titanic_test[\"Embarked\"].unique())\n",
    "\n",
    "# The columns we'll use to predict the target\n",
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(titanic[predictors], titanic.Survived)\n",
    "\n",
    "test_predictions = model.predict(titanic_test[predictors])\n",
    "print(test_predictions)\n",
    "#print(np.transpose([test_predictions]))\n",
    "#print(titanic_test[predictors].shape)\n",
    "#print(test_predictions.shape)\n",
    "print(np.append(titanic_test[predictors], np.transpose([test_predictions]), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kaggle Submission csv - create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     PassengerId  Survived\n",
      "0            892         0\n",
      "1            893         0\n",
      "2            894         0\n",
      "3            895         0\n",
      "4            896         1\n",
      "5            897         0\n",
      "6            898         1\n",
      "7            899         0\n",
      "8            900         1\n",
      "9            901         0\n",
      "10           902         0\n",
      "11           903         0\n",
      "12           904         1\n",
      "13           905         0\n",
      "14           906         1\n",
      "15           907         1\n",
      "16           908         0\n",
      "17           909         0\n",
      "18           910         1\n",
      "19           911         1\n",
      "20           912         0\n",
      "21           913         0\n",
      "22           914         1\n",
      "23           915         1\n",
      "24           916         1\n",
      "25           917         0\n",
      "26           918         1\n",
      "27           919         0\n",
      "28           920         0\n",
      "29           921         0\n",
      "..           ...       ...\n",
      "388         1280         0\n",
      "389         1281         0\n",
      "390         1282         1\n",
      "391         1283         1\n",
      "392         1284         0\n",
      "393         1285         0\n",
      "394         1286         0\n",
      "395         1287         1\n",
      "396         1288         0\n",
      "397         1289         1\n",
      "398         1290         0\n",
      "399         1291         0\n",
      "400         1292         1\n",
      "401         1293         0\n",
      "402         1294         1\n",
      "403         1295         1\n",
      "404         1296         0\n",
      "405         1297         0\n",
      "406         1298         0\n",
      "407         1299         0\n",
      "408         1300         1\n",
      "409         1301         1\n",
      "410         1302         1\n",
      "411         1303         1\n",
      "412         1304         1\n",
      "413         1305         0\n",
      "414         1306         1\n",
      "415         1307         0\n",
      "416         1308         0\n",
      "417         1309         0\n",
      "\n",
      "[418 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Create a new dataframe with only the columns Kaggle wants from the dataset.\n",
    "submission = pandas.DataFrame({\n",
    "        \"PassengerId\": titanic_test[\"PassengerId\"],\n",
    "        \"Survived\": test_predictions\n",
    "    })\n",
    "\n",
    "submission.to_csv(\"kaggle_mshabeer_titanic_test_results_submission.csv\", index=False)\n",
    "\n",
    "print(submission)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.785634118967\n"
     ]
    }
   ],
   "source": [
    "from sklearn import cross_validation\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n",
    "\n",
    "# Initialize our algorithm with the default paramters\n",
    "# n_estimators is the number of trees we want to make\n",
    "# min_samples_split is the minimum number of rows we need to make a split\n",
    "# min_samples_leaf is the minimum number of samples we can have at the place where a tree branch ends (the bottom points of the tree)\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=10, min_samples_split=2, min_samples_leaf=1)\n",
    "\n",
    "num_instances = len(titanic)\n",
    "num_folds = 3\n",
    "seed = 10\n",
    "kfold = cross_validation.KFold(n=num_instances, n_folds = num_folds, random_state = seed) \t\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic.Survived, cv=kfold)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.812584269663\n"
     ]
    }
   ],
   "source": [
    "from sklearn import cross_validation\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n",
    "\n",
    "# Initialize our algorithm with the default paramters\n",
    "# n_estimators is the number of trees we want to make\n",
    "# min_samples_split is the minimum number of rows we need to make a split\n",
    "# min_samples_leaf is the minimum number of samples we can have at the place where a tree branch ends (the bottom points of the tree)\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=10, min_samples_split=2, min_samples_leaf=1)\n",
    "\n",
    "num_instances = len(titanic)\n",
    "num_folds = 10 # increased folds\n",
    "seed = 10\n",
    "kfold = cross_validation.KFold(n=num_instances, n_folds = num_folds, random_state = seed) \t\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic.Survived, cv=kfold)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.81593714927\n"
     ]
    }
   ],
   "source": [
    "# estimators (no of decision trees are increased to 50)\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=50, min_samples_split=4, min_samples_leaf=2)\n",
    "\n",
    "num_instances = len(titanic)\n",
    "num_folds = 3\n",
    "seed = 10\n",
    "kfold = cross_validation.KFold(n=num_instances, n_folds = num_folds, random_state = seed) \t\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic.Survived, cv=kfold)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.837222222222\n"
     ]
    }
   ],
   "source": [
    "# estimators (no of decision trees are increased to 100)\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=100, min_samples_split=4, min_samples_leaf=2)\n",
    "\n",
    "num_instances = len(titanic)\n",
    "num_folds = 20 # increased folds to 20\n",
    "seed = 10\n",
    "kfold = cross_validation.KFold(n=num_instances, n_folds = num_folds, random_state = seed) \t\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic.Survived, cv=kfold)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction - generating new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1\n",
      "1    1\n",
      "2    0\n",
      "3    1\n",
      "4    0\n",
      "dtype: int64\n",
      "0    23\n",
      "1    51\n",
      "2    22\n",
      "3    44\n",
      "4    24\n",
      "Name: Name, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Generating a familysize column\n",
    "titanic[\"FamilySize\"] = titanic.SibSp + titanic.Parch\n",
    "print(titanic.FamilySize.head())\n",
    "\n",
    "# The .apply method generates a new series\n",
    "titanic[\"NameLength\"] = titanic.Name.apply(lambda x: len(x))\n",
    "print(titanic.NameLength.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr          517\n",
      "Miss        182\n",
      "Mrs         125\n",
      "Master       40\n",
      "Dr            7\n",
      "Rev           6\n",
      "Col           2\n",
      "Major         2\n",
      "Mlle          2\n",
      "Countess      1\n",
      "Ms            1\n",
      "Lady          1\n",
      "Jonkheer      1\n",
      "Don           1\n",
      "Mme           1\n",
      "Capt          1\n",
      "Sir           1\n",
      "Name: Name, dtype: int64\n",
      "1     517\n",
      "2     183\n",
      "3     125\n",
      "4      40\n",
      "5       7\n",
      "6       6\n",
      "7       5\n",
      "10      3\n",
      "8       3\n",
      "9       2\n",
      "Name: Name, dtype: int64\n",
      "891\n",
      "891\n",
      "891\n",
      "891\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas\n",
    "\n",
    "# function to extract title from the given name\n",
    "def get_title(name):\n",
    "    # Use a regular expression to extract title. Title starts with alphabet and after end has a period.\n",
    "    title_search = re.search(' ([A-Za-z]+)\\.', name)\n",
    "    # title exists then extract and return.\n",
    "    if title_search is not None:\n",
    "        return title_search.group(1)\n",
    "    return \"\"\n",
    "\n",
    "# Get titles for the names\n",
    "titles = titanic.Name.apply(get_title)\n",
    "# print how often they occur in the data\n",
    "print(pandas.value_counts(titles))\n",
    "\n",
    "# Map each title to an integer.  Some titles are very rare, and are compressed into the same codes as other titles.\n",
    "title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Dr\": 5, \"Rev\": 6, \n",
    "                 \"Major\": 7, \"Col\": 7, \"Mlle\": 8, \"Mme\": 8, \"Don\": 9, \"Lady\": 10, \n",
    "                 \"Countess\": 10, \"Jonkheer\": 10, \"Sir\": 9, \"Capt\": 7, \"Ms\": 2}\n",
    "\n",
    "for k,v in title_mapping.items():\n",
    "    titles[titles == k] = v\n",
    "    \n",
    "# Check if all the titles have been mapped\n",
    "print(pandas.value_counts(titles))\n",
    "\n",
    "# Make sure len of titles array and len of titanic rows are same\n",
    "print(len(titles))\n",
    "print(len(titanic))\n",
    "\n",
    "print(titles.shape[0])\n",
    "print(titanic.shape[0])\n",
    "\n",
    "# Adding a Title column to dataset \n",
    "titanic[\"Title\"]= titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting FamilyGroups feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PassengerId  Survived  Pclass  \\\n",
      "0            1         0       3   \n",
      "1            2         1       1   \n",
      "2            3         1       3   \n",
      "3            4         1       1   \n",
      "4            5         0       3   \n",
      "\n",
      "                                                Name Sex  Age  SibSp  Parch  \\\n",
      "0                            Braund, Mr. Owen Harris   0   22      1      0   \n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...   1   38      1      0   \n",
      "2                             Heikkinen, Miss. Laina   1   26      0      0   \n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)   1   35      1      0   \n",
      "4                           Allen, Mr. William Henry   0   35      0      0   \n",
      "\n",
      "             Ticket     Fare Cabin Embarked  FamilySize  NameLength Title  \\\n",
      "0         A/5 21171   7.2500   NaN        0           1          23     1   \n",
      "1          PC 17599  71.2833   C85        1           1          51     3   \n",
      "2  STON/O2. 3101282   7.9250   NaN        0           0          22     2   \n",
      "3            113803  53.1000  C123        0           1          44     3   \n",
      "4            373450   8.0500   NaN        0           0          24     1   \n",
      "\n",
      "   FamilyId  \n",
      "0        -1  \n",
      "1        -1  \n",
      "2        -1  \n",
      "3        -1  \n",
      "4        -1  \n",
      "-1      800\n",
      " 14       8\n",
      " 149      7\n",
      " 63       6\n",
      " 50       6\n",
      " 59       6\n",
      " 17       5\n",
      " 384      4\n",
      " 27       4\n",
      " 25       4\n",
      " 162      4\n",
      " 8        4\n",
      " 84       4\n",
      " 340      4\n",
      " 43       3\n",
      " 269      3\n",
      " 58       3\n",
      " 633      2\n",
      " 167      2\n",
      " 280      2\n",
      " 510      2\n",
      " 90       2\n",
      " 83       1\n",
      " 625      1\n",
      " 376      1\n",
      " 449      1\n",
      " 498      1\n",
      " 588      1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Mapping of family name to id\n",
    "family_id_mapping = {}\n",
    "\n",
    "current_id =1\n",
    "\n",
    "# Extract family group and assign family id for the given row.\n",
    "def get_family_id(row):\n",
    "    global current_id\n",
    "    #print(row)\n",
    "    # Find the last name\n",
    "    last_name = row[\"Name\"].split(\",\")[0]\n",
    "    \n",
    "    # create a family id \n",
    "    family_id = \"{}{}\".format(last_name, row[\"FamilySize\"])\n",
    "    \n",
    "    # look up in the family id mapping\n",
    "    if family_id not in family_id_mapping:\n",
    "        family_id_mapping[family_id] = current_id\n",
    "        current_id += 1\n",
    "    \n",
    "    return family_id_mapping[family_id]\n",
    "\n",
    "print(titanic.head())\n",
    "# For each row in titanic array, compute the family_id\n",
    "family_ids = titanic.apply(get_family_id, axis=1)\n",
    "\n",
    "# For families with familysize < 3, make them independent i.e no family.\n",
    "family_ids[ titanic[\"FamilySize\"] < 3 ] = -1\n",
    "\n",
    "# Print count of unique id\n",
    "print(pandas.value_counts(family_ids))\n",
    "\n",
    "titanic[\"FamilyId\"] = family_ids\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding The Best Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW0AAAEsCAYAAAAIBeLrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8e7dbdacd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.83393258427\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectKBest,f_classif\n",
    "\n",
    "# Updated predictors with new features extracted.\n",
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\", \"FamilySize\", \"Title\", \"FamilyId\"]\n",
    "\n",
    "k = 5\n",
    "# perform feature selection - i.e select k features which are highly correlated with target variable - Survived.\n",
    "selector = SelectKBest(score_func = f_classif, k = k)\n",
    "selector.fit(titanic[predictors], titanic.Survived)\n",
    "\n",
    "# Get raw p_values for each feature and transform them to scores\n",
    "scores = -np.log10(selector.pvalues_)\n",
    "\n",
    "x_coordinates = range(len(predictors))\n",
    "y_coordinates = scores\n",
    "\n",
    "plt.bar(x_coordinates, y_coordinates)\n",
    "plt.xticks(x_coordinates, predictors, rotation='vertical') # labels are shown vertical\n",
    "plt.show()\n",
    "\n",
    "# 4 top scoring features are selected for training a RandomForest\n",
    "predictors = ['Pclass', 'Sex', 'Fare', 'Title']\n",
    "\n",
    "# min_samples_split is the minimum number of rows we need to make a split\n",
    "# min_samples_leaf is the minimum number of samples we can have at the place where a tree branch ends (the bottom points of the tree)\n",
    "model = RandomForestClassifier(n_estimators = 25, min_samples_split=8, min_samples_leaf= 4, random_state = 1)\n",
    "\n",
    "kfold = cross_validation.KFold(n=len(titanic),\n",
    "                               n_folds = 10,\n",
    "                               random_state = 1)\n",
    "\n",
    "scores = cross_validation.cross_val_score(model, titanic[predictors], titanic.Survived, cv = kfold)\n",
    "print(scores.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ensembling  - Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.819304152637\n",
      "Voting ensemble, training accuracy mean:0.814814814815\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.cross_validation import KFold\n",
    "import numpy as np\n",
    "\n",
    "# The algorithms we want to ensemble.\n",
    "# We're using the more linear predictors for the logistic regression, and everything with the gradient boosting classifier.\n",
    "algorithms = [\n",
    "    [GradientBoostingClassifier(random_state=1, \n",
    "                                n_estimators=25, \n",
    "                                max_depth=3), \n",
    "     [\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Embarked\", \"FamilySize\", \"Title\", \"FamilyId\"]\n",
    "    ],\n",
    "    [LogisticRegression(random_state=1), \n",
    "     [\"Pclass\", \"Sex\", \"Fare\", \"FamilySize\", \"Title\", \"Age\", \"Embarked\"]]\n",
    "]\n",
    "\n",
    "# Initialize the cross validation folds\n",
    "kf = KFold(titanic.shape[0], n_folds=3, random_state=1)\n",
    "\n",
    "##### Approach 1 - Averaging probabilities ####\n",
    "predictions = []\n",
    "for train, test in kf:\n",
    "    train_target = titanic[\"Survived\"].iloc[train]\n",
    "    full_test_predictions = []\n",
    "    # Make predictions for each algorithm on each fold\n",
    "    for alg, predictors in algorithms:\n",
    "        # Fit the algorithm on the training data.\n",
    "        alg.fit(titanic[predictors].iloc[train,:], train_target)\n",
    "        # Select and predict on the test fold.  \n",
    "        # The .astype(float) is necessary to convert the dataframe to all floats and avoid an sklearn error.\n",
    "        test_predictions = alg.predict_proba(titanic[predictors].iloc[test,:].astype(float))[:,1]\n",
    "        full_test_predictions.append(test_predictions)\n",
    "    # Use a simple ensembling scheme -- just average the predictions to get the final classification.\n",
    "    test_predictions = (full_test_predictions[0] + full_test_predictions[1]) / 2\n",
    "    # Any value over .5 is assumed to be a 1 prediction, and below .5 is a 0 prediction.\n",
    "    test_predictions[test_predictions <= .5] = 0\n",
    "    test_predictions[test_predictions > .5] = 1\n",
    "    predictions.append(test_predictions)\n",
    "\n",
    "# Put all the predictions together into one array.\n",
    "predictions = np.concatenate(predictions, axis=0)\n",
    "\n",
    "# Compute accuracy by comparing to the training data.\n",
    "accuracy = sum(predictions[predictions == titanic[\"Survived\"]]) / len(predictions)\n",
    "print(accuracy)\n",
    "\n",
    "##### Approach 2 - Majority voting ######################\n",
    "estimators = []\n",
    "estimators.append(('gbc', GradientBoostingClassifier(random_state=1, \n",
    "                                n_estimators=25, \n",
    "                                max_depth=3)))\n",
    "estimators.append(('lr', LogisticRegression(random_state=1)))\n",
    "\n",
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Embarked\", \"FamilySize\", \"Title\", \"FamilyId\"]\n",
    "X = titanic[predictors]\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "ensemble = VotingClassifier(estimators)\n",
    "\n",
    "results = cross_validation.cross_val_score(ensemble, X, titanic.Survived, cv=kf)\n",
    "print(\"Voting ensemble, training accuracy mean:{}\".format(results.mean()))\n",
    "\n",
    "######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching Our Changes On The Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1     240\n",
      "2      79\n",
      "3      72\n",
      "4      21\n",
      "7       2\n",
      "6       2\n",
      "10      1\n",
      "5       1\n",
      "Name: Title, dtype: int64\n",
      "{\"O'Sullivan0\": 426, 'Mangan0': 620, 'Lindqvist1': 543, 'Denkoff0': 297, 'Sincock0': 813, 'Rouse0': 413, 'Berglund0': 207, 'Meo0': 142, 'Arnold-Franchi1': 49, 'Chronopoulos1': 71, 'Skoog5': 63, 'Walcroft0': 897, 'Widener2': 329, 'Pengelly0': 217, 'Goncalves0': 400, 'Andersen0': 829, 'Myhrman0': 626, 'Beane1': 456, 'Moss0': 104, 'Carlsson0': 610, 'Nicholls2': 136, 'Jussila1': 110, 'Jussila0': 483, 'Peltomaki0': 725, 'Long0': 632, 'Cassebeer0': 809, 'Portaluppi0': 858, 'Wheadon0': 33, 'Connolly0': 261, 'Hansen2': 680, 'Stephenson1': 493, 'Howard0': 862, 'Smyth0': 805, 'Davies0': 336, 'Silven2': 359, 'Vanden Steen0': 311, 'Sadowitz0': 878, 'Astor1': 571, 'Patchett0': 480, 'Denbury0': 891, 'Johanson0': 184, 'Coleridge0': 220, 'Christmann0': 87, 'Carter3': 340, 'Compton2': 665, 'Carter1': 226, 'Turkula0': 414, 'Lindeberg-Lind0': 793, 'Hassab0': 558, 'Badman0': 755, 'Aronsson0': 914, 'Saad0': 566, 'Mellors0': 208, 'Mamee0': 36, 'Dika0': 734, 'Madsen0': 119, 'Bird0': 801, 'Anderson0': 395, 'Kraeff0': 42, 'Robbins0': 468, 'Lundahl0': 522, 'Gilinski0': 490, 'Porter0': 107, 'Sdycoff0': 352, 'Green0': 204, 'Bishop1': 263, 'Sinkkonen0': 603, 'Otter0': 640, 'Dahl0': 299, 'Troutt0': 582, 'Samaan2': 48, 'Vartanian0': 877, 'Edvardsson0': 553, 'Larsson-Rondberg0': 920, 'Angheloff0': 874, 'Petroff0': 98, 'Burke0': 134, 'Cardeza1': 556, 'Hawksford0': 599, 'Somerton0': 416, 'Healy0': 248, 'Andersson0': 137, 'Fortune5': 27, 'Andersson6': 14, 'Willard0': 842, 'Johnson2': 9, 'Johnson0': 273, 'Wheeler0': 913, 'Coxon0': 91, 'Banfield0': 696, 'Wilkes1': 702, 'Rowe0': 883, 'Thomson0': 834, 'McCarthy0': 7, 'Panula5': 50, 'West3': 58, 'Kallio0': 374, 'Hoyt1': 206, 'Hoyt0': 638, 'Mannion0': 589, 'Touma2': 232, 'Futrelle1': 4, 'Jardin0': 507, 'Rosenbaum0': 827, 'Lemore0': 437, 'Davies2': 460, 'Myles0': 703, 'Reynolds0': 835, 'Robert1': 630, 'Butt0': 451, 'Wick2': 286, 'Emanuel0': 628, 'Ibrahim Shawah0': 643, 'Carbines0': 175, 'McEvoy0': 583, 'Moutal0': 75, 'Loring0': 875, 'Giles0': 896, 'Ling0': 157, 'Watson0': 552, 'Lahoud0': 443, 'Sedgwick0': 302, 'Assaf0': 711, 'Storey0': 799, 'Beesley0': 22, 'Sheerlinck0': 79, 'Hunt0': 218, 'Duquemin0': 800, 'Clifford0': 408, 'Stranden0': 602, 'Ahlin1': 40, 'Woolner0': 55, 'Caram1': 482, 'Sandstrom2': 11, 'Nakid2': 333, 'Frost0': 412, 'Moen0': 73, 'Henry0': 239, 'Persson1': 241, 'Mudd0': 671, 'Glynn0': 32, 'Bowerman1': 312, 'Douglas2': 816, 'Daniel0': 505, 'Nysten0': 132, 'Saalfeld0': 270, 'Turcin0': 173, 'Nasser1': 10, 'Waelens0': 78, 'Khalil1': 753, 'Pokrnic0': 863, 'Homer0': 502, 'Graham1': 242, 'Graham0': 699, 'Brown0': 177, 'Dorking0': 256, 'Clark1': 851, 'Bing0': 72, 'Blank0': 191, 'Tornquist0': 245, 'Olsen1': 180, 'Olsen0': 144, 'Lane0': 815, 'Davison1': 304, 'Butler0': 544, 'Calderhead0': 575, 'Cor0': 530, 'Kalvik0': 534, 'Hagland1': 386, 'Boulos2': 131, 'Boulos0': 497, 'Niklasson0': 855, 'Leitch0': 496, 'Endres0': 581, 'Fleming0': 275, 'Natsch1': 247, 'Garfirth0': 614, 'MacKay0': 854, 'Blackwell0': 300, 'Thayer2': 461, 'Holm0': 657, 'Rekic0': 105, 'Allen0': 5, 'Hendekovic0': 282, 'Svensson0': 424, 'Montvila0': 698, 'Perreault0': 441, 'Francatelli0': 278, 'Gronnestad0': 621, 'Maisner0': 399, 'Radeff0': 537, 'Daly0': 432, 'Hiltunen2': 846, 'Conlon0': 921, 'Jarvis0': 488, 'Bowen0': 515, 'Holthen0': 770, 'Risien0': 453, 'Walker0': 436, 'Klaber0': 578, 'Wright0': 466, 'Artagaveytia0': 419, 'Navratil2': 138, 'Hays2': 658, 'Seward0': 383, 'Hays0': 279, 'Appleton2': 478, 'Fry0': 654, 'Romaine0': 171, 'de Messemaeker1': 469, 'Gustafsson0': 331, 'Gustafsson2': 101, 'Lulic0': 659, 'Beckwith2': 225, 'Sutton0': 516, 'Cunningham0': 355, 'Carver0': 780, 'Drapkin0': 790, 'Hocking0': 840, 'Behr0': 700, 'Cotterill0': 911, 'Rothschild1': 434, 'Stankovic0': 257, 'Abelson1': 277, 'Shaughnessy0': 727, 'Lobb1': 230, 'Shellard0': 423, 'Knight0': 593, 'Silverthorne0': 572, \"O'Leary0\": 535, 'Saundercock0': 13, 'Baxter1': 114, 'Elias0': 624, 'Nankoff0': 598, 'Mitkoff0': 533, 'Lines1': 677, 'Burns0': 298, \"O'Driscoll0\": 47, 'Beattie0': 778, 'Cumings1': 2, 'Culumovic0': 674, 'Thomas2': 769, 'Penasco y Castellana1': 276, 'Parkes0': 251, 'Duff Gordon1': 467, 'Sloper0': 24, 'Strom2': 228, 'Ringhini0': 327, 'Barah0': 616, 'Richards2': 350, 'Crease0': 67, 'Cavendish1': 600, 'Earnshaw1': 797, 'Hilliard0': 795, 'Stengel1': 766, 'Lamb0': 752, 'Bjornstrom-Steffansson0': 371, 'Masselmani0': 20, 'Moubarek2': 65, 'Turja0': 555, 'Goldsmith2': 154, 'Fahlstrom0': 210, 'Holverson1': 35, 'Herman3': 510, \"O'Donoghue0\": 756, 'Nenkoff0': 205, 'Nysveen0': 292, 'Eklund0': 617, 'Paulner0': 487, 'Whabee0': 895, 'Nilsson0': 284, 'Dick1': 563, 'Dantcheff0': 639, 'Swift0': 682, 'Humblen0': 570, 'Matinoff0': 798, 'McCormack0': 661, 'Duane0': 253, 'Ekstrom0': 121, 'Wiseman0': 366, 'Smith0': 160, 'Krekorian0': 881, 'Nourney0': 922, 'Abrahamsson0': 850, 'Saether0': 928, 'Marvin1': 604, 'Cleaver0': 576, 'Gill0': 683, 'Cameron0': 193, 'Augustsson0': 663, 'Petranec0': 97, 'Stahelin-Maeglin0': 523, 'McDermott0': 80, 'Wittevrongel0': 873, 'Dean3': 90, 'Laleff0': 691, 'Meyer0': 649, 'Meyer1': 34, 'Chaudanson0': 733, 'Johansson0': 100, 'Cacic0': 405, 'Minkoff0': 740, 'Ryerson4': 280, 'Becker3': 167, 'Murphy0': 824, 'Tobin0': 627, 'Murphy1': 219, 'Goldschmidt0': 93, 'Kink2': 68, 'Gilbert0': 917, 'Weisz1': 125, 'Hold1': 215, 'Lingane0': 821, 'Stanley0': 420, 'Doyle0': 748, 'Minahan2': 222, 'Toufik0': 450, 'Minahan1': 354, 'Olsvigen0': 559, 'Odahl0': 307, 'Coleff0': 435, 'Case0': 750, 'Vestrom0': 15, 'Brobeck0': 782, 'Hood0': 70, 'Yousseff0': 421, 'Evans0': 776, 'Mardirosian0': 869, 'Abrahim0': 706, 'Barton0': 109, 'Icard0': 61, 'Smiljanic0': 148, 'Harmer0': 634, 'Collett0': 826, 'Ford4': 84, 'Maioni0': 428, 'Abbing0': 675, 'Ford0': 870, 'Kink-Heilmann4': 918, 'Oreskovic0': 347, 'Sundman0': 356, 'Kink-Heilmann2': 168, 'Lehmann0': 339, 'Dennis0': 288, 'del Carlo1': 316, 'Najib0': 690, 'Ohman0': 465, 'McCoy2': 272, 'Osen0': 129, 'Pernot0': 166, 'Kent0': 415, 'Turpin1': 41, 'Zabour1': 108, 'Alexander0': 650, 'Palsson4': 8, 'Hogeboom1': 618, 'Keefe0': 404, 'Eustis1': 422, 'Marechal0': 669, 'White0': 879, 'Uruchurtu0': 30, 'Riordan0': 923, 'Downton0': 485, 'Ilmakangas1': 591, 'Corn0': 147, 'Maybery0': 817, 'Parrish1': 236, 'Jerwan0': 406, 'Aks1': 678, 'Lam0': 565, 'Slayter0': 290, 'Weir0': 567, 'Frolicher2': 454, 'Danbom2': 365, 'Alhomaki0': 670, 'Brown2': 548, 'Faunthorpe1': 53, 'Wilson0': 908, 'Givard0': 195, 'Leyson0': 213, 'Potter1': 692, 'Emir0': 26, 'Slocovski0': 85, 'Celotti0': 86, 'Rood0': 169, 'Betros0': 330, 'Spinner0': 785, 'Larsson0': 211, 'Goldsmith0': 723, 'Andreasson0': 88, 'Vande Walle0': 183, 'Zakarian0': 788, 'Morley0': 396, 'Trout0': 344, 'Jefferys2': 716, 'Lundstrom0': 893, 'Thorneycroft1': 372, 'Shorney0': 92, 'Attalah0': 111, 'Ward0': 235, 'Bowenur0': 783, 'Tucker0': 738, 'Yousif0': 310, 'Wiklund1': 325, 'Thorne0': 233, 'Warren0': 861, 'Warren1': 320, 'Milling0': 398, 'Delalic0': 828, 'Rommetvedt0': 545, 'Pickard0': 370, 'Torfa0': 812, 'Hassan0': 592, 'Heininen0': 655, 'Vendel0': 844, 'Miles0': 745, 'Bazzani0': 200, 'Plotcharsky0': 335, 'Lindell1': 503, 'Caldwell2': 76, 'Meanwell0': 474, 'Bystrom0': 684, 'Harrison0': 238, 'Goldenberg1': 388, 'Webber0': 117, 'Shelley1': 693, 'Mayne0': 577, 'Honkanen0': 198, 'Deacon0': 831, 'Karlsson0': 410, 'Ismay0': 909, 'Asplund0': 837, 'Calic0': 153, 'Eitemiller0': 538, 'Chapman0': 568, 'Asplund6': 25, 'Dulles0': 888, 'Peuchen0': 385, 'Cairns0': 244, 'Midtsjo0': 857, 'Bailey0': 611, 'Hanna0': 268, 'Garside0': 481, 'Mernagh0': 179, 'Staneff0': 74, 'Toomey0': 393, 'Howard1': 710, 'Canavan0': 425, 'Kennedy0': 781, 'Dakic0': 560, 'Bucknell0': 728, 'Foo0': 529, 'Pasic0': 666, 'Veal0': 819, 'Novel0': 57, 'Lemberopolous0': 673, 'Hocking4': 625, 'Aldworth0': 747, 'Moran1': 106, 'Abbott2': 252, 'Kvillner0': 377, 'Elias2': 309, 'Gracie0': 786, 'Doharr0': 476, 'Norman0': 472, 'Pearce0': 806, 'Braf0': 764, 'Leader0': 641, 'Smart0': 402, 'White1': 99, 'Gale1': 348, 'de Brito0': 890, 'Swane0': 773, 'Doling1': 95, 'Sap0': 720, 'Moor1': 607, 'Pedersen0': 758, 'Taussig2': 237, 'Pinsky0': 174, 'Mulvihill0': 739, 'Gallagher0': 573, 'Lockyer0': 901, 'Thomas0': 777, 'Markun0': 694, 'de Mulder0': 258, 'Kassem0': 444, 'Yrois0': 182, 'Lundin0': 802, 'Kantor1': 96, 'Sobey0': 126, 'Shutes0': 506, 'Brocklebank0': 509, 'Parker0': 866, 'Cherry0': 234, 'Dooley0': 701, 'Hedman0': 646, 'Madigan0': 181, 'Ovies y Rodriguez0': 742, 'Strilic0': 904, 'Parr0': 524, 'Campbell0': 401, 'Mullens0': 569, 'Maguire0': 889, 'Charters0': 363, 'Baimbrigge0': 822, 'Jonsson0': 477, 'Ostby1': 54, 'Brady0': 715, 'Wilhelms0': 551, 'Williams1': 145, 'Williams0': 18, 'Maenpaa0': 221, 'Stone0': 662, 'Birnbaum0': 761, 'Lennon1': 46, 'Olsson0': 254, 'Harknett0': 214, 'Horgan0': 508, 'Hart0': 353, 'Hart2': 283, 'Ilett0': 82, 'Hellstrom0': 810, 'Naughton0': 924, 'Baumann0': 156, 'Youseff0': 185, 'Reeves0': 240, 'Thomas1': 645, 'Jensen0': 527, 'Jensen1': 584, 'Cribb1': 150, 'Ross0': 486, 'Andersen-Jensen1': 176, 'Gilnagh0': 146, 'Moran0': 6, 'Coelho0': 123, 'Razi0': 679, 'Harbeck0': 910, 'Shine0': 775, 'Reuchlin0': 660, 'Kiernan1': 196, 'Partner0': 295, 'Jermyn0': 322, 'Salkjelsvik0': 103, 'Lahtinen2': 281, 'Drew2': 358, 'Hosono0': 260, 'Danoff0': 289, 'van Billiard2': 143, 'Bracken0': 203, 'Dintcheff0': 787, 'Coutts2': 305, 'Connors0': 113, 'Colbert0': 919, 'Jenkin0': 69, 'Hodges0': 586, 'Vovk0': 442, 'Omont0': 825, 'Backstrom1': 188, 'Devaney0': 44, 'Backstrom3': 83, 'Barbara1': 317, \"O'Connell0\": 520, 'Vande Velde0': 608, 'Reynaldo0': 380, 'Isham0': 163, 'Peruschitz0': 807, \"O'Brien0\": 463, \"O'Brien1\": 170, 'Giglio0': 130, 'Ponesell0': 644, 'Nicola-Yarred1': 39, 'Angle1': 439, 'Mellinger1': 246, 'Newell1': 197, 'Davidson3': 759, 'Lang0': 431, 'Davidson1': 549, 'Richard0': 127, 'Oxenham0': 868, 'Ball0': 293, 'Nasr0': 872, 'Schabert1': 779, 'Drazenoic0': 122, 'Robins1': 124, 'Richards5': 376, 'Chevre0': 726, 'Salander0': 852, 'Stanton0': 774, 'Nosworthy0': 51, 'Moore0': 116, 'Newsom2': 128, 'Sjoblom0': 635, 'Zimmerman0': 364, 'Kenyon1': 392, 'Fynney0': 21, 'Laroche3': 43, 'Renouf1': 409, 'Silvey1': 375, 'Riihivouri0': 905, 'Christy2': 484, 'Renouf3': 588, 'Pulbaum0': 730, 'Theobald0': 612, 'Mahon0': 833, 'Pekoniemi0': 112, 'Spector0': 926, 'Bateman0': 140, 'Spencer1': 31, 'Gibson1': 906, 'Allison3': 269, 'Cohen0': 186, 'Dimic0': 306, 'Karnes0': 849, 'Bourke2': 172, 'Farthing0': 447, 'Hamalainen2': 224, 'Karaic0': 504, 'Hyman0': 848, 'Pettersson0': 648, 'Landergren0': 328, 'Willey0': 532, 'Braund1': 1, 'Rosenshine0': 886, 'Slemen0': 652, 'Snyder1': 709, 'Jalsevac0': 390, 'Harder1': 324, 'Oliva y Ocana0': 927, 'Elsbury0': 494, 'Assam0': 885, 'Carrau0': 81, 'Dyker1': 757, 'Sutehall0': 697, 'Everett0': 839, 'Sawyer0': 554, 'Vander Planke1': 19, 'Peters0': 557, 'Vander Planke3': 794, 'Vander Planke2': 38, 'Dahlberg0': 695, 'Sharp0': 462, 'Sirota0': 667, 'McCaffry0': 864, 'Demetri0': 751, 'Barkworth0': 521, 'Buckley0': 771, 'Mack0': 623, 'Petersen0': 784, 'Sjostedt0': 212, 'Slabenoff0': 499, 'Dodge2': 382, 'Mineff0': 266, 'Lithman0': 811, 'Hocking3': 449, 'Hickman2': 115, 'Perkin0': 194, 'Sweet0': 841, 'Stewart0': 64, 'Giles1': 681, 'Lindblom0': 250, 'Meek0': 357, 'Morrow0': 470, 'Newell2': 539, 'Beavan0': 326, 'Balkic0': 688, 'van Melkebeke0': 687, 'Foreman0': 387, 'Hampe0': 378, 'Birkeland0': 351, 'Ware0': 903, 'Mockler0': 315, 'Millet0': 391, 'Peter2': 120, 'Yasbeck1': 512, 'Osman0': 642, 'Buss0': 337, 'Sunderland0': 202, 'Fillbrook0': 892, 'Candee0': 836, 'Lefebre4': 162, 'Bryhl1': 590, 'McGovern0': 314, 'Nye0': 66, 'Davis0': 525, 'Aubart0': 323, 'Spedden2': 287, 'Harris0': 201, 'Harris1': 62, 'McGough0': 433, 'Hewlett0': 16, 'Cann0': 37, 'Makinen0': 763, 'Mallet2': 656, 'Mock1': 717, 'Kirkland0': 517, 'Ridsdale0': 446, 'Connaghton0': 605, 'Stoytcheff0': 475, 'Ashby0': 915, 'Hippach1': 294, 'Dowdell0': 77, 'Klasen2': 161, 'Assaf Khalil0': 712, 'Rothes0': 613, 'Nancarrow0': 765, \"O'Connor0\": 394, 'Ivanoff0': 597, 'Clarke1': 367, 'Scanlan0': 403, 'Daniels0': 791, 'Troupiansky0': 595, 'Johansson Palmquist0': 768, 'Geiger0': 743, 'Reed0': 227, 'Rintamaki0': 492, 'Chambers1': 587, 'Malachard0': 876, 'Haas0': 265, 'Corey0': 737, 'Greenfield1': 94, 'Brandeis0': 808, 'Jansson0': 341, 'Crafton0': 796, 'McNeill0': 838, 'Gheorgheff0': 362, 'Lindstrom0': 847, 'Mitchell0': 550, 'Dibden0': 899, 'Bonnell0': 12, 'Murdlin0': 491, 'Lester0': 651, 'Van Impe2': 361, 'Simonius-Blumer0': 531, 'Torber0': 501, 'Linehan0': 843, 'Gillespie0': 585, 'Naidenoff0': 259, 'McNamee1': 601, 'de Pelsmaeker0': 255, 'Quick2': 429, 'Bentham0': 856, 'Byles0': 139, 'Salomon0': 820, 'Sage10': 149, 'Baccos0': 845, 'Frolicher-Stehli2': 489, 'Wirz0': 704, 'Peduzzi0': 389, 'Chibnall1': 155, 'Hakkarainen1': 133, 'Watt0': 151, 'Sirayanian0': 60, 'Leonard0': 165, 'Bissette0': 243, 'Adahl0': 319, 'LeRoy0': 452, 'Tomlin0': 653, 'Smith1': 729, 'Kimball1': 513, 'Gavey0': 511, 'Mionoff0': 102, 'Farrell0': 445, 'Goodwin7': 59, 'Strom1': 187, 'Ali0': 192, 'Funk0': 313, 'Van der hoef0': 158, 'Roebling0': 686, 'Stokes0': 898, 'Simmons0': 473, 'Asim0': 318, 'Rosblom2': 231, 'Willer0': 772, 'Saade0': 865, 'Lovell0': 209, 'Phillips0': 368, 'Phillips1': 818, 'Lyntakoff0': 859, 'Leinonen0': 526, 'Jones0': 708, 'Markoff0': 676, 'Levy0': 264, 'Frauenthal2': 540, 'Frauenthal1': 296, 'Rheims0': 871, 'Finoli0': 830, 'Wenzel0': 853, 'Baclini3': 384, 'Johannesen-Bratthammer0': 381, 'Jonkoff0': 609, 'Williams-Lambert0': 308, 'Serepeca0': 672, 'Cornell2': 746, 'Longley0': 518, \"O'Dwyer0\": 28, 'Enander0': 887, 'Chisholm0': 860, 'Kelly0': 271, 'Douglas1': 457, 'Crosby2': 455, 'Niskanen0': 345, 'Collander0': 301, 'Gee0': 397, 'Tikkanen0': 334, 'McKane0': 342, 'Molson0': 418, 'Bostandyeff0': 519, 'Ayoub0': 631, 'Rogers0': 45, 'Chip0': 668, 'Cook0': 546, 'Stead0': 229, 'Nesson0': 882, 'Peacock2': 804, 'Soholt0': 580, 'Moraweck0': 285, 'Rasmussen0': 823, 'Barry0': 754, 'Bidois0': 332, 'Taylor1': 547, 'Bradley0': 430, 'Fischer0': 561, 'Barber0': 262, 'Julian0': 900, 'Albimona0': 189, 'Hale0': 164, 'Flynn0': 369, 'Sadlier0': 338, 'Keane0': 274, 'Pallas y Castello0': 907, 'Young0': 291, 'Lindahl0': 223, 'Rugg0': 56, 'Johnston3': 633, 'Guest0': 760, 'Petterson1': 379, 'Pavlovic0': 440, 'Abelseth0': 732, 'Sivic0': 471, 'Hirvonen1': 411, 'Hirvonen2': 705, 'Allum0': 664, 'Tenglin0': 762, 'Head0': 832, 'Keeping0': 744, 'Straus1': 749, 'Harrington0': 500, 'Borebank0': 803, 'Beauchamp0': 792, 'Windelov0': 417, 'Lesurer0': 596, 'Pain0': 343, 'Lievens0': 622, 'Fox0': 303, 'Lewy0': 267, 'McMahon0': 118, 'Botsford0': 894, 'Greenberg0': 579, 'Heikkinen0': 3, 'Henriksson0': 925, 'Mangiavacchi0': 731, 'Louch1': 373, 'Chapman1': 495, 'Schmidt0': 789, 'Berriman0': 594, 'Franklin0': 722, 'Bjorklund0': 736, 'Moussa0': 321, 'Payne0': 916, 'Ware1': 867, 'Hee0': 721, 'Ryan0': 438, \"O'Keefe0\": 902, 'Roth0': 719, 'Madill1': 562, 'Duran y More1': 685, 'Foley0': 767, 'Bengtsson0': 152, 'Harper1': 52, 'Kilgannon0': 629, 'Badt0': 541, 'Salonen0': 448, 'Guggenheim0': 636, 'Widegren0': 349, 'Brewe0': 619, 'Sivola0': 159, 'Nirva0': 615, 'Rice5': 17, 'Andrews0': 647, 'Andrews1': 249, 'Collyer2': 216, 'Strandberg0': 407, 'Colley0': 542, 'Nicholson0': 458, 'Nieminen0': 741, 'Chaffee1': 89, 'Gaskell0': 637, 'McCrie0': 814, 'Wells2': 606, 'Leeni0': 464, 'Todoroff0': 29, 'Flegenheim0': 713, 'McCrae0': 735, 'Adams0': 346, 'Corbett0': 724, 'Hipkins0': 912, 'Pears1': 141, 'Sagesser0': 528, 'Carr0': 190, 'Ilieff0': 707, 'McGowan0': 23, 'Hansen0': 514, 'Hansen1': 574, 'Karun1': 564, 'Rush0': 479, 'Katavelas0': 718, 'Matthews0': 360, 'Laitinen0': 427, 'Kreuchen0': 884, 'Hagardon0': 880, 'Daher0': 714, 'Padro y Manent0': 459, 'Andrew0': 135, 'Vander Cruyssen0': 689, 'Jacobsohn1': 199, 'Lurette0': 178, 'Jacobsohn3': 498, 'Hegarty0': 536}\n",
      "0    16\n",
      "1    32\n",
      "2    25\n",
      "3    16\n",
      "4    44\n",
      "Name: NameLength, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# First, we'll add titles to the test set.\n",
    "titles = titanic_test[\"Name\"].apply(get_title)\n",
    "# We're adding the Dona title to the mapping, because it's in the test set, but not the training set\n",
    "title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Dr\": 5, \"Rev\": 6, \"Major\": 7, \"Col\": 7, \"Mlle\": 8, \"Mme\": 8, \"Don\": 9, \"Lady\": 10, \"Countess\": 10, \"Jonkheer\": 10, \"Sir\": 9, \"Capt\": 7, \"Ms\": 2, \"Dona\": 10}\n",
    "for k,v in title_mapping.items():\n",
    "    titles[titles == k] = v\n",
    "titanic_test[\"Title\"] = titles\n",
    "# Check the counts of each unique title.\n",
    "print(pandas.value_counts(titanic_test[\"Title\"]))\n",
    "\n",
    "# Now, we add the family size column.\n",
    "titanic_test[\"FamilySize\"] = titanic_test[\"SibSp\"] + titanic_test[\"Parch\"]\n",
    "\n",
    "# Now we can add family ids.\n",
    "# We'll use the same ids that we did earlier.\n",
    "print(family_id_mapping)\n",
    "\n",
    "family_ids = titanic_test.apply(get_family_id, axis=1)\n",
    "family_ids[titanic_test[\"FamilySize\"] < 3] = -1\n",
    "titanic_test[\"FamilyId\"] = family_ids\n",
    "\n",
    "#### Adding NameLength feature to titanic_test ####\n",
    "# The .apply method generates a new series\n",
    "titanic_test[\"NameLength\"] = titanic_test.Name.apply(lambda x: len(x))\n",
    "print(titanic_test.NameLength.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting On The Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.11905703  0.50546179  0.12405561  0.1300521   0.51102313]\n",
      "[ 0.  1.  0.  0.  1.]\n",
      "[0 1 0 0 1]\n",
      "     PassengerId  Survived\n",
      "0            892         0\n",
      "1            893         1\n",
      "2            894         0\n",
      "3            895         0\n",
      "4            896         1\n",
      "5            897         0\n",
      "6            898         1\n",
      "7            899         0\n",
      "8            900         1\n",
      "9            901         0\n",
      "10           902         0\n",
      "11           903         0\n",
      "12           904         1\n",
      "13           905         0\n",
      "14           906         1\n",
      "15           907         1\n",
      "16           908         0\n",
      "17           909         0\n",
      "18           910         1\n",
      "19           911         1\n",
      "20           912         0\n",
      "21           913         1\n",
      "22           914         1\n",
      "23           915         0\n",
      "24           916         1\n",
      "25           917         0\n",
      "26           918         1\n",
      "27           919         0\n",
      "28           920         0\n",
      "29           921         0\n",
      "..           ...       ...\n",
      "388         1280         0\n",
      "389         1281         0\n",
      "390         1282         0\n",
      "391         1283         1\n",
      "392         1284         1\n",
      "393         1285         0\n",
      "394         1286         0\n",
      "395         1287         1\n",
      "396         1288         0\n",
      "397         1289         1\n",
      "398         1290         0\n",
      "399         1291         0\n",
      "400         1292         1\n",
      "401         1293         0\n",
      "402         1294         1\n",
      "403         1295         0\n",
      "404         1296         0\n",
      "405         1297         0\n",
      "406         1298         0\n",
      "407         1299         0\n",
      "408         1300         1\n",
      "409         1301         1\n",
      "410         1302         1\n",
      "411         1303         1\n",
      "412         1304         1\n",
      "413         1305         0\n",
      "414         1306         1\n",
      "415         1307         0\n",
      "416         1308         0\n",
      "417         1309         0\n",
      "\n",
      "[418 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Embarked\", \"FamilySize\", \"Title\", \"FamilyId\"]\n",
    "\n",
    "algorithms = [\n",
    "    [GradientBoostingClassifier(random_state=1, n_estimators=25, max_depth=3), predictors],\n",
    "    [LogisticRegression(random_state=1), [\"Pclass\", \"Sex\", \"Fare\", \"FamilySize\", \"Title\", \"Age\", \"Embarked\"]]\n",
    "]\n",
    "\n",
    "full_predictions = []\n",
    "for alg, predictors in algorithms:\n",
    "    # Fit the algorithm using the full training data.\n",
    "    alg.fit(titanic[predictors], titanic[\"Survived\"])\n",
    "    # Predict using the test dataset.  We have to convert all the columns to floats to avoid an error.\n",
    "    predictions = alg.predict_proba(titanic_test[predictors].astype(float))[:,1]\n",
    "    full_predictions.append(predictions)\n",
    "\n",
    "# The gradient boosting classifier generates better predictions, so we weight it higher.\n",
    "predictions = (full_predictions[0] * 3 + full_predictions[1]) / 4\n",
    "\n",
    "print(predictions[0:5])\n",
    "\n",
    "###########  Instructions ######\n",
    "# 1. Turn the predictions into either 0 or 1 by turning the predictions less than or equal to .5 into 0, \n",
    "# and the predictions greater than .5 into 1.\n",
    "\n",
    "predictions[predictions <= 0.5 ] = 0\n",
    "predictions[predictions > 0.5 ] = 1\n",
    "\n",
    "print(predictions[0:5])\n",
    "\n",
    "# 2. Then, convert the predictions to integers using the .astype(int) method -- if you don't, \n",
    "# Kaggle will give you a score of 0.\n",
    "\n",
    "predictions = predictions.astype(int)\n",
    "print(predictions[0:5])\n",
    "\n",
    "# 3. Finally, create a submission dataframe where the first column is PassengerId, and the second column \n",
    "# is Survived (this will be the predictions).\n",
    "######################\n",
    "\n",
    "submission = pandas.DataFrame({\n",
    "    'PassengerId' : titanic_test[\"PassengerId\"],\n",
    "    'Survived'    : predictions\n",
    "})\n",
    "\n",
    "submission.to_csv(\"kaggle_mshabeer_titanic_test_results_submission_improved.csv\" , index=False)\n",
    "print(submission)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
